{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This is the second in a series of tutorials that illustrate how to download the IRS 990 e-file data available at https://aws.amazon.com/public-data-sets/irs-990/\n",
    "\n",
    "Specifically, in this notebook we will download into a MongoDB table the main IRS 990 filings as well as the associated \"schedules.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://pandas.pydata.org/pandas-docs/stable/options.html\n",
    "pd.set_option('display.max_columns', None)  #Set PANDAS to show all columns in DataFrame\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB\n",
    "Depending on the project, I will store the data in SQLite or MongoDB. This time I'll use MongoDB -- it's great for storing JSON data where each observation could have different variables. Before we get to the interesting part the following code blocks set up the MongoDB environment and the new database we'll be using. \n",
    "\n",
    "**_Note:_** In a terminal we'll have to start MongoDB by running the command *mongod* or *sudo mongod*. Then we run the following code block here to access MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Connect to our database that contains the filing index data we downloaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE MONGODB DATABASE\n",
    "db = client['irs_990_db']\n",
    "\n",
    "# DEFINE THE COLLECTIONS WHERE I'LL INSERT THE DATA\n",
    "file_list_2011 = db['file_list_2011']\n",
    "file_list_2012 = db['file_list_2012']\n",
    "file_list_2013 = db['file_list_2013']\n",
    "file_list_2014 = db['file_list_2014']\n",
    "file_list_2015 = db['file_list_2015']\n",
    "file_list_2016 = db['file_list_2016']\n",
    "file_list_2017 = db['file_list_2017']\n",
    "file_list_2018 = db['file_list_2018']\n",
    "file_list_2019 = db['file_list_2019']\n",
    "file_list_2020 = db['file_list_2020']\n",
    "file_list_2021 = db['file_list_2021']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Check how many observations in the database tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203075\n",
      "261622\n",
      "261449\n",
      "387529\n",
      "261034\n",
      "378420\n",
      "489013\n",
      "457510\n",
      "416910\n",
      "333722\n",
      "461887\n"
     ]
    }
   ],
   "source": [
    "print(file_list_2011.estimated_document_count())\n",
    "print(file_list_2012.estimated_document_count())\n",
    "print(file_list_2013.estimated_document_count())\n",
    "print(file_list_2014.estimated_document_count())\n",
    "print(file_list_2015.estimated_document_count())\n",
    "print(file_list_2016.estimated_document_count())\n",
    "print(file_list_2017.estimated_document_count())\n",
    "print(file_list_2018.estimated_document_count())\n",
    "print(file_list_2019.estimated_document_count())\n",
    "print(file_list_2020.estimated_document_count())\n",
    "print(file_list_2021.estimated_document_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> We will try to download all of the *990* filings and omit the *990EZ* and *990PF* filings. For example, in the 2011 index file we have 113,018 990 filings to download. We'll grab all of these as well as all those from the 2012 through 2021 index files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': '990', 'count': 113018},\n",
       " {'_id': '990EZ', 'count': 65858},\n",
       " {'_id': '990PF', 'count': 24199}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bson.son import SON\n",
    "pipeline = [ {\"$group\": {\"_id\": \"$FormType\", \"count\": {\"$sum\": 1}}} ]\n",
    "list(file_list_2011.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over List of Filings, grab e-file data, and insert into second database\n",
    "First we'll write a function to turn an ordered dictionary (which is what is returned by *xmltodict*) into a normal Python dictionary so that we can combine it with the filing data gathered above.\n",
    "\n",
    "What I'm doing in the next five or so blocks of code is looping over the *list* of 3 million plus filings that I've previously downloaded, and then downloading the full e-file 990 filings for all orgs that match one of our 8,304 EINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads, dumps\n",
    "from collections import OrderedDict\n",
    "\n",
    "def to_dict(input_ordered_dict):\n",
    "    return loads(dumps(input_ordered_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>In order to give you a sense of what we're going to do I'll set a list of two sample EINs and work through a few key steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eins = ['340090940', '742547528']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Check how many filings there are for this sample of 2 EINs. Here we are accessing the 2021 <code>file_list</code> collection in MongoDB and printing out the two rows in the database that match the EINs in the above list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('617c359548ae60d313c44618'), 'EIN': '742547528', 'TaxPeriod': '201912', 'DLN': '93493319016300', 'FormType': '990', 'URL': 'https://s3.amazonaws.com/irs-form-990/202003199349301630_public.xml', 'OrganizationName': 'AUXILIARY TO THE PASO DEL NORTE CHILDRENS DEVELOPMENT CENTER', 'SubmittedOn': '2021-03-31', 'ObjectId': '202003199349301630', 'LastUpdated': '2021-06-11T13:10:10'} \n",
      "\n",
      "{'_id': ObjectId('617c359748ae60d313c67eb9'), 'EIN': '340090940', 'TaxPeriod': '201912', 'DLN': '93493321126220', 'FormType': '990', 'URL': 'https://s3.amazonaws.com/irs-form-990/202023219349312622_public.xml', 'OrganizationName': 'INTERNATIONAL BROTHERHOOD TEAMSTERS 293 TCWH', 'SubmittedOn': '2021-06-24', 'ObjectId': '202023219349312622', 'LastUpdated': '2021-09-08T16:51:44'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in file_list_2021.find({'EIN': { '$in': eins}})[:2]:\n",
    "    print(f, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To recall, our current dataset contains basic details on all 3,912,171 990 filings. We still don't have the actual 990 data, however. To get that, we will have to pick which filings we want and then access the *URL* column for that filing as seen in our dataset. What we are going to want to do later on is loop over all the rows in our database and access each filing by visiting the link shown in the *URL* field. Here we can print out the *URL* field value in the 2021 index file for our two sample EINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/irs-form-990/201903199349320465_public.xml \n",
      "\n",
      "https://s3.amazonaws.com/irs-form-990/201903199349320485_public.xml \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in  file_list_2020.find({'EIN': { '$in': eins}})[:2]:\n",
    "    print(file['URL'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To access the actual Form 990 filing we are going to use a Python library called <code>urllib2</code>. In the following code block we access the first matching row in our MongoDB collection, visit the *URL* and assign the page data to a variable called *url_data*, read in the data and assign it to a variable called *f_string*, and then print out the first 1000 characters of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/irs-form-990/201903199349320465_public.xml \n",
      "\n",
      "b'\\xef\\xbb\\xbf<?xml version=\"1.0\" encoding=\"utf-8\"?>\\r\\n<Return xmlns=\"http://www.irs.gov/efile\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.irs.gov/efile\" returnVersion=\"2018v3.1\">\\r\\n  <ReturnHeader binaryAttachmentCnt=\"0\">\\r\\n    <ReturnTs>2019-11-15T15:12:19-06:00</ReturnTs>\\r\\n    <TaxPeriodEndDt>2018-12-31</TaxPeriodEndDt>\\r\\n    <PreparerFirmGrp>\\r\\n      <PreparerFirmEIN>341322309</PreparerFirmEIN>\\r\\n      <PreparerFirmName>\\r\\n        <BusinessNameLine1Txt>CIUNI &amp; PANICHI INC</BusinessNameLine1Txt>\\r\\n      </PreparerFirmName>\\r\\n      <PreparerUSAddress>\\r\\n        <AddressLine1Txt>25201 CHAGRIN BLVD 200</AddressLine1Txt>\\r\\n        <CityNm>CLEVELAND</CityNm>\\r\\n        <StateAbbreviationCd>OH</StateAbbreviationCd>\\r\\n        <ZIPCd>441225683</ZIPCd>\\r\\n      </PreparerUSAddress>\\r\\n    </PreparerFirmGrp>\\r\\n    <ReturnTypeCd>990</ReturnTypeCd>\\r\\n    <TaxPeriodBeginDt>2018-01-01</TaxPeriodBeginDt>\\r\\n    <Filer>\\r\\n      <EIN>340090940</EIN>\\r\\n      <BusinessName>\\r\\n        <Busine'\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import urllib.request\n",
    "for file in file_list_2020.find({'EIN': { '$in': eins}})[:1]:\n",
    "    print(file['URL'], '\\n')\n",
    "    url_data = urllib.request.urlopen(file['URL'])\n",
    "    f_string = url_data.read()\n",
    "    print(f_string[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The above data are in XML format. We will need to convert them to JSON format in order to insert into MongoDB. For that we will leverage the Python module <code>xmltodict</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/irs-form-990/201903199349320465_public.xml\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import urllib.request\n",
    "for file in file_list_2020.find({'EIN': { '$in': eins}})[:1]:\n",
    "    print(file['URL'])\n",
    "    url_data = urllib.request.urlopen(file['URL'])\n",
    "    f_string = url_data.read()\n",
    "    data = xmltodict.parse(f_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Now the data are in a dictionary format. Let's check which keys are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Return'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We see there is nothing relevant there, so let's drop down a level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['@xmlns', '@xmlns:xsi', '@xsi:schemaLocation', '@returnVersion', 'ReturnHeader', 'ReturnData'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The first four keys do no contain much useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.irs.gov/efile'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return']['@xmlns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.w3.org/2001/XMLSchema-instance'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return']['@xmlns:xsi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.irs.gov/efile'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return']['@xsi:schemaLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018v3.1'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return']['@returnVersion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The <code>ReturnHeader</code> key contains 12 sub-keys. Here we have information that could be useful for certain research projects. For most purposes, however, we can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['@binaryAttachmentCnt', 'ReturnTs', 'TaxPeriodEndDt', 'PreparerFirmGrp', 'ReturnTypeCd', 'TaxPeriodBeginDt', 'Filer', 'BusinessOfficerGrp', 'PreparerPersonGrp', 'FilingSecurityInformation', 'TaxYr', 'BuildTS'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return']['ReturnHeader'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Lastly, we inspect the <code>ReturnData</code> key. The first key simply provides a count of the number of documents as well as keys holding the 990 return data and any of the associated \"schedules.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['@documentCnt', 'IRS990', 'IRS990ScheduleD', 'IRS990ScheduleO', 'IRS990ScheduleR'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return']['ReturnData'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>In almost all cases, what we want is the 990 data. As we can see here, there are 191 keys nested under the <code>IRS990</code> key. \n",
    "\n",
    "*Note:* - In Python 2 this is a list but in Python 3 it's a (non-iterable) *odict* so I convert it to a list first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of keys in 990 data: 191 \n",
      "\n",
      "First 5 keys: ['@documentId', '@referenceDocumentId', 'PrincipalOfficerNm', 'USAddress', 'GrossReceiptsAmt']\n"
     ]
    }
   ],
   "source": [
    "print(\"# of keys in 990 data:\", len(data['Return']['ReturnData']['IRS990'].keys()), '\\n')\n",
    "print(\"First 5 keys:\", list(data['Return']['ReturnData']['IRS990'].keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@documentId',\n",
       " '@referenceDocumentId',\n",
       " 'PrincipalOfficerNm',\n",
       " 'USAddress',\n",
       " 'GrossReceiptsAmt']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['Return']['ReturnData']['IRS990'].keys())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>In many cases, such as <code>TotalAssetsGrp</code>, there are multiple keys nested under it. Depending on your data needs you can \"flatten\" these data as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('BOYAmt', '594548'), ('EOYAmt', '598835')])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Return']['ReturnData']['IRS990']['TotalAssetsGrp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>In brief, what we want to do is write code that will loop over all relevant rows in our MongoDB collection, visit the respective URL where the 990 data are located, grab those data, convert them to a dictionary, and then insert into a new MongoDB collection. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up new MongoDB collection\n",
    "In this second collection we will be inserting the actual 990 data we will be downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MY COLLECTION (DATABASE TABLE) WHERE I'LL INSERT THE MESSAGES \n",
    "filings_990 = db['filings_990']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Set a unique constraint on the collection based on *URL*. This will avert duplicates from being inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.filings_990.create_index([('URL', pymongo.ASCENDING)], unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id_', 'URL_1']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(db.filings_990.index_information())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>There were no filings yet before first download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings_990.estimated_document_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download IRS 990 Data and Available Schedules\n",
    "Now let's run the loop for all rows in all collections, grab the IRS 990 filings, and insert them into our new MongoDB table. This block has some additional code that I won't discuss in detail (see comments below for further details). The short answer is that we are looping over each row of each collection in our database, visiting the URL that contains the 990 data, and then grabbing all of the data returned by the <code>IRS990</code> key. For convenience purposes, we then combine this new data with the associated filing index data from our first database, and then insert the combined data into our new <code>filings_990</code> collection.\n",
    "\n",
    "Note that this code block will only work for organizations that have the <code>IRS990</code> key. The check is found in the following line of code:\n",
    "\n",
    "&nbsp; &nbsp; <code>if 'IRS990' in data['Return']['ReturnData']:</code>\n",
    "\n",
    "This means that organizations filing *990EZ* or *990PF* will be skipped. However, the code block could easily be modified to grab 990EZ or 990PF filings. \n",
    "\n",
    "Compared to the <a href=\"http://social-metrics.org/irs-990-e-file-data-part-4/\">simpler version of this tutorials</a>, we are also adding columns for five keys that are nested under the ['Return'] key. If you're not familiar with Python this line of code from the code block we'll be using will seem perplexing:\n",
    "\n",
    "&nbsp; &nbsp; <code>return_info = {k:v for k,v in datax['Return'].iteritems() if k not in ('ReturnData')}</code>\n",
    "\n",
    "What this code does is assign to a new dictionary called *return_info* every key and value nested under &nbsp; <code>datax['Return']</code> &nbsp;  except for <code>datax['Return]['ReturnData']</code>. &nbsp; The latter contains our 990 data, so we are going to deal with that separately. So, *return_info* will contain one column for each of the following keys: &nbsp; <code>'@xmlns', '@xmlns:xsi', '@xsi:schemaLocation', '@returnVersion', 'ReturnHeader'</code>. &nbsp; We are not likely to use these data but it will be good to have them handy in case we need them. \n",
    "\n",
    "Similarly, with the following line we create a new dictionary that will contain all of the keys nested under &nbsp; <code>datax['Return']['ReturnData']</code> &nbsp; except for &nbsp; <code>datax['Return]['ReturnData']['IRS990]</code>. &nbsp; \n",
    "\n",
    "&nbsp; &nbsp; <code>schedules = {k:v for k,v in datax['Return']['ReturnData'].iteritems() if k not in ('IRS990')}</code>\n",
    "\n",
    "We then combine the 9 columns from our existing MongoDB collection, the 5 columns from *return_info*, the available columns in the dictionary *schedules*, and the 200+ columns from &nbsp; <code>datax['Return]['ReturnData']['IRS990]</code>. &nbsp;  This combined dictionary is then added to our new MongoDB *filings* collection.\n",
    "\n",
    "What we are doing here is deciding not to \"flatten\" the data available in the schedules. Instead, the focus of this dataset is the actual core 990 data. Our resultant dataset will have a dozen or so \"background\" or filing detail columns, a half-dozen or so columns containing data on any available schedules, and then 200+ columns containing the 990 data. For most research purposes this will suffice and will minimize the need to flatten keys with nested data. But whenever we have the need the data will be there waiting for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**_Note_**:\n",
    "\n",
    "- This code will take a long time to download close to 2 million 990 filings. While I could have created a loop that would iterate of the 2011 through 2021 index files in turn, I have chosen to ddo this manually in the following loop. So, once you've successfully downloaded all the 990 filings in the 2011 index file, change the line that reads \n",
    "    - *for f in file_list_2011.find({'FormType': '990'}, no_cursor_timeout=True).batch_size(10)[counter:]:*\n",
    "- For example, change it 2012, rerun the loop, then continue on to 2013, etc.\n",
    "    - *for f in file_list_2012.find({'FormType': '990'}, no_cursor_timeout=True).batch_size(10)[counter:]:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('counter: ', 199946, 'CASA OF EL PASO INC') \n",
      "\n",
      "# of minutes:  1048.0079850383336 \n",
      "\n",
      "Total # of filings in dnatabase: 2016601\n",
      "# of filings added to database: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import xmltodict\n",
    "#import urllib3\n",
    "import urllib.request\n",
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "from IPython.display import display, clear_output   ##### FOR USE WITH STDOUT (DYNAMIC, SINGLE-LINE PRINTING)\n",
    "\n",
    "start_count = filings_990.estimated_document_count()\n",
    "counter = 1-1\n",
    "#I HAVE CHANGED THIS TO ONLY LOOP OVER THE FILES IN THE INDEX THAT ARE '990' INSTEAD OF 990PF OR 990EZ\n",
    "#I STILL KEEP THE if 'IRS990' in datax BECAUSE SOME RETURNS ARE EMPTY\n",
    "for f in file_list_2011.find({'FormType': '990'}, no_cursor_timeout=True).batch_size(10)[counter:]:   \n",
    "    counter += 1\n",
    "    print(f)\n",
    "    if 'URL' in f:\n",
    "        url = f['URL']\n",
    "        print(url, '\\n')\n",
    "        try:\n",
    "            url_data = urllib.request.urlopen(url)\n",
    "            f_string = url_data.read()\n",
    "            datax = xmltodict.parse(f_string)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            if 'IRS990' in datax['Return']['ReturnData']: ##### CHECK WHETHER THE 'IRS990' KEY IS PRESENT IN THE DATA\n",
    "                ##### CREATE DICTIONARY CONTAINING ALL KEYS NESTED UNDER 'Return' EXCEPT FOR THE 'ReturnData' KEY\n",
    "                ##### THERE WILL BE FIVE KEYS IN return_info: '@xmlns', '@xmlns:xsi', '@xsi:schemaLocation', '@returnVersion', 'ReturnHeader'\n",
    "                #####NEXT LINE IS PYTHON 2 WAY\n",
    "                #return_info = {k:v for k,v in datax['Return'].iteritems() if k not in ('ReturnData')}\n",
    "                return_info = {k:v for k,v in datax['Return'].items() if k not in ('ReturnData')}\n",
    "                \n",
    "                ##### CREATE DICTIONARY CONTAINING ALL KEYS IN ['ReturnData'] EXCEPT FOR THE 'IRS990' KEY\n",
    "                ##### THERE WILL BE A KEY CALLED '@documentCount' (OR '@documentCnt') AS WELL AS ONE PER INCLUDED SCHEDULE\n",
    "                #####NEXT LINE IS PYTHON 2 WAY \n",
    "                #schedules = {k:v for k,v in datax['Return']['ReturnData'].iteritems() if k not in ('IRS990')}\n",
    "                schedules = {k:v for k,v in datax['Return']['ReturnData'].items() if k not in ('IRS990')}\n",
    "                \n",
    "                ##### CREATE DICTIONARY FOR 990 DATA\n",
    "                data = datax['Return']['ReturnData']['IRS990']\n",
    "                data = to_dict(data)\n",
    "                        \n",
    "                ##### COMBINE THE DICT OF FILING INFO FROM FIRST STEP WITH FILING DATA GATHERED HERE\n",
    "                ##### NEXT LINE IS PYTHON 2; THE ONE AFTER IS MY FIRST CRACK AT PYTHON 3\n",
    "                #c = {key: value for (key, value) in (f.items() + return_info.items() + schedules.items() + data.items())}\n",
    "                #c = {key: value for (key, value) in dict(**f, **return_info, **schedules, **data)}\n",
    "                c = c = dict(**f, **return_info, **schedules, **data)\n",
    "                c.pop('_id', None)        #DROP 'id' (OR IT WILL NOT INSERT)\n",
    "                t = json.dumps(c)\n",
    "                #print t \n",
    "                loaded_entry = json.loads(t) \n",
    "                #print type(loaded_entry) , loaded_entry    #<type 'dict'>\n",
    "                try:\n",
    "                    filings_990.insert_one(loaded_entry)\n",
    "                except pymongo.errors.DuplicateKeyError:# , e:\n",
    "                    print(\"*****THERE IS A DUPLICATEKEYERROR*****\")#(e, '\\n')\n",
    "        \n",
    "        except KeyError:\n",
    "            print('IRS9990 key not in data (likely a 990EZ or 990PF filer)')\n",
    "            #print(data['Return']['ReturnData'].keys())\n",
    "            print(data.keys())\n",
    "              \n",
    "    else:\n",
    "        print(f['IsAvailable'])\n",
    "         \n",
    "    clear_output()\n",
    "    print(('counter: ', counter, f['OrganizationName']), '\\n')\n",
    "    sys.stdout.flush()        \n",
    "        \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('# of minutes: ', elapsed/60, '\\n')\n",
    "print(\"Total # of filings in dnatabase:\", filings_990.estimated_document_count())\n",
    "print(\"# of filings added to database:\", filings_990.estimated_document_count()  - start_count, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# of minutes: ', 1.6810495116666668)\n",
      "('Number of columns:', 1)\n",
      "('Number of observations:', 1617926)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://s3.amazonaws.com/irs-form-990/201100709349300510_public.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   URL\n",
       "0  https://s3.amazonaws.com/irs-form-990/201100709349300510_public.xml"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and save a list of downloaded 990 filings\n",
    "There is a possibility that not all of the filings downloaded. Accordingly, we are going to create a list of all filings that were successfully inserted into our new collection. As a first step, we will create a PANDAS dataframe, called ``df``,  into which we will insert all of the *URL* fields from our *filings_990* collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(list(filings_990.find({}, {'URL':1, \n",
    "    '_id':0})))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Number of columns:\", len(df.columns))\n",
    "print(\"Number of observations:\", len(df))\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded = df['URL'].tolist()\n",
    "print len(downloaded)\n",
    "downloaded[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('downloaded_990_filings.json', 'w') as fp:\n",
    "    json.dump(downloaded, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun download on non-downloaded filings\n",
    "Run this on all index files from 2011 through 2021. Manually change ``for f in file_list....`` line as you did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('counter: ', 2763336, u'GLOBAL MEDIA OUTREACH') \n",
      "\n",
      "# of minutes:  659.30174687 \n",
      "\n",
      "# of filings added to database: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import xmltodict\n",
    "import urllib2\n",
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "from IPython.display import display, clear_output   ##### FOR USE WITH STDOUT (DYNAMIC, SINGLE-LINE PRINTING)\n",
    "\n",
    "start_count = filings_990.count_documents({})\n",
    "counter = 1-1\n",
    "\n",
    "for f in file_list_2011.find({}, no_cursor_timeout=True).batch_size(10)[counter:]:    \n",
    "    counter += 1\n",
    "    \n",
    "    print f\n",
    "    if 'URL' in f:\n",
    "        url = f['URL']\n",
    "        print url, '\\n'\n",
    "        if url not in downloaded:\n",
    "            print 'Original -- will try to download>>>>>>>>>>>>'\n",
    "            url_data = urllib2.urlopen(url)\n",
    "            f_string = url_data.read()\n",
    "            datax = xmltodict.parse(f_string)\n",
    "        \n",
    "            try:\n",
    "                if 'IRS990' in datax['Return']['ReturnData']: ##### CHECK WHETHER THE 'IRS990' KEY IS PRESENT IN THE DATA\n",
    "                    ##### CREATE DICTIONARY CONTAINING ALL KEYS NESTED UNDER 'Return' EXCEPT FOR THE 'ReturnData' KEY\n",
    "                    ##### THERE WILL BE FIVE KEYS IN return_info: '@xmlns', '@xmlns:xsi', '@xsi:schemaLocation', '@returnVersion', 'ReturnHeader'\n",
    "                    return_info = {k:v for k,v in datax['Return'].iteritems() if k not in ('ReturnData')}\n",
    "                    ##### CREATE DICTIONARY CONTAINING ALL KEYS IN ['ReturnData'] EXCEPT FOR THE 'IRS990' KEY\n",
    "                    ##### THERE WILL BE A KEY CALLED '@documentCount' (OR '@documentCnt') AS WELL AS ONE PER INCLUDED SCHEDULE\n",
    "                    schedules = {k:v for k,v in datax['Return']['ReturnData'].iteritems() if k not in ('IRS990')}\n",
    "                    ##### CREATE DICTIONARY FOR 990 DATA\n",
    "                    data = datax['Return']['ReturnData']['IRS990']\n",
    "                    data = to_dict(data)\n",
    "                        \n",
    "                    ##### COMBINE THE DICT OF FILING INFO FROM FIRST STEP WITH FILING DATA GATHERED HERE\n",
    "                    c = {key: value for (key, value) in (f.items() + return_info.items() + schedules.items() + data.items())}\n",
    "                    c.pop('_id', None)        #DROP 'id' (OR IT WILL NOT INSERT)\n",
    "                    t = json.dumps(c)\n",
    "                    #print t \n",
    "                    loaded_entry = json.loads(t) \n",
    "                  #print type(loaded_entry) , loaded_entry    #<type 'dict'>\n",
    "                    try:\n",
    "                        filings_990.insert_one(loaded_entry)\n",
    "                    except pymongo.errors.DuplicateKeyError, e:\n",
    "                        print e, '\\n'\n",
    "        \n",
    "            except KeyError:\n",
    "                print 'IRS9990 key not in data (likely a 990EZ or 990PF filer)'\n",
    "                print data['Return']['ReturnData'].keys()\n",
    "        else:\n",
    "            print '....FILING HAS ALREADY BEEN DOWNLOADED>>>>>'\n",
    "             \n",
    "    else:\n",
    "        print f['IsAvailable']\n",
    "         \n",
    "    clear_output()\n",
    "    print ('counter: ', counter, f['OrganizationName']), '\\n'\n",
    "    sys.stdout.flush()        \n",
    "        \n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print '# of minutes: ', elapsed/60, '\\n'\n",
    "print \"# of filings added to database:\", filings_990.count_documents({}) - start_count, '\\n'              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>As noted above, our code will only work for organizations that have the <code>IRS990</code> key. We won't get data for 990EZ or 990PF filers. Focusing on one of the three groups of filers would be the typical research design decision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
